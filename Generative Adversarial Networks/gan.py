# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jynSApva3rBdhQha7O8jT9cZ488k8TXy
"""

import tensorflow as tf
from keras.models import Model
from keras.layers import Dense, BatchNormalization, Input, Dropout, LeakyReLU
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import os
import numpy as np

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = 2*(x_train/255.0) - 1
x_test = 2*(x_test/255.0) - 1

print(x_train.shape)

N, H, W = x_train.shape
D = H*W
x_train = x_train.reshape(-1, D)
x_test = x_test.reshape(-1, D)

print(x_train.shape)

latent_dim = 100
batch_size = 32
epochs = 30000
d_losses = []
g_losses = []
gen_sample_for_epoch = 200

ones = np.ones(batch_size)
zeros = np.zeros(batch_size)

if not os.path.exists('gan_images'):
  os.makedirs('gan_images')

def gen_generator(latent_dim):
  i = Input(shape=(latent_dim,))
  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)
  x = BatchNormalization(momentum=0.7)(x)
  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)
  x = BatchNormalization(momentum=0.7)(x)
  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)
  x = BatchNormalization(momentum=0.7)(x)
  x = Dense(D, activation=tf.keras.activations.tanh)(x)

  return Model(i, x)

def gen_discriminator(D):
  i = Input(shape=(D,))
  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)
  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)
  x = Dense(1, activation=tf.keras.activations.sigmoid)(x)

  return Model(i, x)

discriminator = gen_discriminator(D)
discriminator.compile(optimizer=Adam(0.0002, 0.5), loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])

generator = gen_generator(latent_dim)

z = Input(shape=(latent_dim,))
fake_img = generator(z)

discriminator.trainable = False

fake_img_output = discriminator(fake_img)

combined_model = Model(z, fake_img_output)
combined_model.compile(optimizer=Adam(0.0002, 0.5), loss=tf.keras.losses.binary_crossentropy)

def sample_images(epoch):
  rows, cols = 5, 5
  noise = np.random.randn(rows*cols, latent_dim)
  imgs = generator.predict(noise)

  imgs = 0.5 * imgs + 0.5

  fig, axs = plt.subplots(rows, cols)
  idx = 0
  for i in range(rows):
    for j in range(cols):
      axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')
      axs[i,j].axis('off')
      idx += 1
  fig.savefig("gan_images/%d.png" % epoch)
  plt.close()

for epoch in range(epochs):
  ## Train discriminator##
  real_img_id = np.random.randint(0, x_train.shape[0], batch_size)
  real_imgs = x_train[real_img_id]

  fake_noise = np.random.randn(batch_size, latent_dim)
  fake_imgs = generator.predict(fake_noise)

  discriminator.trainable = True
  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)
  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)
  d_loss = (d_loss_real+d_loss_fake)/2.0
  d_acc = (d_acc_real+d_acc_fake)/2.0
  discriminator.trainable = False

  ## Train combined ##
  fake_noise = np.random.randn(batch_size, latent_dim)
  g_loss = combined_model.train_on_batch(fake_noise, ones)

  fake_noise = np.random.randn(batch_size, latent_dim)
  g_loss = combined_model.train_on_batch(fake_noise, ones)

  d_losses.append(d_loss)
  g_losses.append(g_loss)

  if epoch % 100 == 0:
    print(f"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \
      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}")

  if epoch % gen_sample_for_epoch == 0:
    sample_images(epoch)

plt.plot(g_losses, label='g_losses')
plt.plot(d_losses, label='d_losses')
plt.legend()
plt.show()